#데이터 불러오기
data("iris")
head(iris)

#패키지 설치 및 나이브베이즈 분석실시
##각 변수별 사전확률 및 조건부 확률 계산
library(e1071)
b<-naiveBayes(Species~., data=iris)
b

# 예측을 통한 사후 확률 계산
pb<-predict(b,newdata=iris)
pb

## 정오분류표표 계산하기
t<-table(pb, iris$Species)
t
sum(diag(t))/sum(t)
1-sum(diag(t))/sum(t)

library(caret)
cpb<-confusionMatrix(pb,reference=iris$Species,positive='1')
cpb


#예제 1 decision Tree모델과 Naïve Bayes모델의 오분류표를 비교하여라
##decidon Tree
library(rpart)
data(iris)
iris
mp<-rpart(Species~., data=iris)
summary(mp)
pmp<-predict(mp,newdata=iris,type="class")
pmp_con<-confusionMatrix(pmp,iris$Species)
pmp_con$table
nb_model <- naiveBayes(Species ~ ., data = iris)
nb_pred <- predict(nb_model, newdata = iris)
nb_cm <- confusionMatrix(nb_pred, iris$Species)
nb_cm$table
# Naïve Bayes 모델이 Decision Tree 모델보다 더 뛰어난 성능을 보임을 의미

#예제 2 data(spam)을 나이브베이즈 분류하고, klaR을 이용하여 시각화 하라
url <- "https://cran.r-project.org/src/contrib/Archive/ElemStatLearn/ElemStatLearn_2015.6.26.2.tar.gz"
install.packages(url, repos = NULL, type = "source")
install.packages("ElemStatLearn")
library(ElemStatLearn)
data("spam")
str(spam)
train_index <- sample(1:nrow(spam), 0.7 * nrow(spam))
train_data <- spam[train_index, ]
test_data <- spam[-train_index, ]
model_nb <- naiveBayes(type ~ ., data = train_data)
pred_nb <- predict(model_nb, newdata = test_data)
confusionMatrix(pred_nb, test_data$type)
model_nb_klaR <- NaiveBayes(type ~ ., data = spam)


#예제 3 data(housevote)를 나이브베이즈 분류하라
#klaR의 NaiveBayes 사용 (e1071과 다름)
library(klaR)
library(caret)
data(housevote)
train_index <- sample(1:nrow(housevote), floor(0.7 * nrow(housevote)), replace = FALSE)
train_data <- housevote[train_index, ]
test_data <- housevote[-train_index, ]
model_nb <- NaiveBayes(party ~ ., data = train_data, usekernel = FALSE)
pred_nb <- predict(model_nb, newdata = test_data)
predicted_classes <- pred_nb$class
confusionMatrix(predicted_classes, test_data$party)
plot(model_nb)